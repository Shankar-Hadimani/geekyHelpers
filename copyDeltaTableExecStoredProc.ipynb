{"cells":[{"cell_type":"code","source":["### Import libraries, required\nfrom multiprocessing.pool import ThreadPool\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import DataFrameWriter\nfrom pyspark.sql.functions import lit, monotonically_increasing_id, row_number, max\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport json\n\n### pool size\npool = ThreadPool(10)\n\n### Set parameters / arguments\n\n\ndbutils.widgets.text(\"p_SourceSystem\", \"\", \"Source System Name\")\np_SourceSystem = dbutils.widgets.get(\"p_SourceSystem\")\n\ndbutils.widgets.text(\"p_SubSystem\", \"\", \"Sub System Name\")\np_SubSystem = dbutils.widgets.get(\"p_SubSystem\")\n\ndbutils.widgets.text(\"p_Phase\", \"\", \"PhaseName/StageName\")\np_Phase = dbutils.widgets.get(\"p_Phase\")\n\n# assign constant arguments\nc_stg_dbname =\"raw\"\nc_src_file_format =\"delta\"\nc_delta_load = \"DELTA\"\nc_full_load = \"FULL\"\nc_ref_load = \"REF\"\nc_snapshot_load = \"SNAPSHOT\"\nc_partial_load = \"PARTIAL\"\nc_max_num_rec = 100000\n\n\n### environment variables\nv_process_start_dtm =datetime.utcnow().isoformat(sep=' ', timespec='seconds')\nv_job_loc = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nv_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\nv_job_name = v_job_loc.rsplit('/',1)[1]\nv_ActivityName='DeltaLake: '+spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\") \nv_ActivityType='DeltaMerge'\n\n### generate a unique number\ndt = datetime.now()\nseq_Id = int(dt.strftime(\"%Y%m%d%H%M%S\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2b217f7-c8d5-4f12-884e-e2741ac9e82c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["jdbcConnStrUrl = dbutils.secrets.get(scope = \"bpdevtestie1kv001\", key = \"bp-asql-bpdevtestie1sqldb004-jdbc-secret\")\nconfig_query = \"\"\"(SELECT \n  [SourceName], \n  [System], \n  [SubSystem], \n  [TableName], \n  [LoadType], \n  [SourceDBName], \n  [SourceLocation], \n  [DestinationLocation],\n  ADLMergeKey,\n  ADLMergeSQL,\n  [WatermarkColumn],\n  CONVERT(VARCHAR(25), [WatermarkValue], 120) AS  [WatermarkValue],\n  [SourceSQL],\n  [StgActive]\nFROM \n  [config].[IngestTableList]  \n   INNER JOIN [audit].[Watermark] \n   ON LOWER([SubSystem])=LOWER([WatermarkSubSystem])\n   AND LOWER([TableName])=LOWER([WatermarkTableName])\n   AND [Active] = 1\n   AND UPPER([SourceName]) = '{SRC_SYS}'\n   AND UPPER([SubSystem]) = '{SRC_SUBSYS}') as IngestTableList \"\"\".format(SRC_SYS=p_SourceSystem, SRC_SUBSYS = p_SubSystem)\n\n#Read metadata configuration table list from SQL database\nconfig_table_df = spark.read.jdbc(url=jdbcConnStrUrl, table=config_query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9db7d563-1923-485b-bc8d-401213611c64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["\nclass SqlDB:\n  db_conn_string= jdbcConnStrUrl\n  \n  def __init__(self):\n    self.driver_manager = spark._sc._gateway.jvm.java.sql.DriverManager\n    self.con = self.driver_manager.getConnection(jdbcConnStrUrl)\n\n  def execute_spoc_without_output(self,spoc_statement):\n    # Create callable statement\n    exec_statement = self.con.prepareCall(spoc_statement)\n        \n    # Execute the statement\n    result = exec_statement.execute()\n        \n    # Connections close\n    exec_statement.close()\n    self.con.close()\n    \n    return result\n  \n  def execute_spoc_with_output(self,spoc_statement):\n    # Create callable statement\n    exec_statement = self.con.prepareCall(spoc_statement)\n        \n    # Register the output parameters with their index and datatype (indexing starts at 1)\n    exec_statement.registerOutParameter(1, spark._sc._gateway.jvm.java.sql.Types.INTEGER)\n        \n    # Execute the statement\n    result = exec_statement.execute()\n        \n    # Fetch the result with the correct method (get<javaType>)\n    result = exec_statement.getInt(1) \n    \n    # Connections close\n    exec_statement.close()\n    self.con.close()\n    \n    return result\n     \n  def execute_query(sql_statement):\n    #Read metadata configuration table list from SQL database\n    config_table_df = spark.read.jdbc(url= jdbcConnStrUrl , table=sql_statement)\n    return config_table_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2c6622c-c1cc-4f1b-b366-0a7ce3f561d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def file_exists(path):\n  if path[:5] == \"/dbfs\":\n    import os\n    return os.path.exists(path)\n  else:\n    try:\n      dbutils.fs.ls(path)\n      return True\n    except Exception as e:\n      if 'java.io.FileNotFoundException' in str(e):\n        return False\n      else:\n        raise"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bfa0cc0-cfc6-48d4-83f0-aca9c44cf9ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def search_database(database):\n  if len([(i) for i in spark.catalog.listDatabases() if i.name==str(database)]) != 0:\n    return True\n  return False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cda8b18-0841-4c0c-a678-df965c6874d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def search_object(database, table):\n\tif len([(i) for i in spark.catalog.listTables(database) if i.name==str(table)]) != 0:\n\t\treturn True\n\treturn False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50d36f99-4753-411c-af97-c0b3ef5f173c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def get_row_processed_cnt(path , activityID):\n  import databricks.koalas as ks\n  from delta.tables import DeltaTable\n  \n  # set deltalake spark path\n  deltaTable = DeltaTable.forPath(spark, path)\n  # Gather audit details\n  df = deltaTable.history(1)\n  merge_record_cnt = df.select(df[\"operationMetrics\"][\"numTargetRowsInserted\"],\n                                       df[\"operationMetrics\"][\"numTargetRowsUpdated\"], \n                                       df[\"operationMetrics\"][\"numTargetRowsDeleted\"],\n                                       df[\"operationMetrics\"][\"numSourceRows\"],\n                                       df[\"operationMetrics\"][\"numOutputRows\"])\n  num_rec_processed = merge_record_cnt.to_koalas().to_numpy()\n  numTargetRowsInserted = int(num_rec_processed[0][0]) if num_rec_processed[0][0] is not None else 0\n  numTargetRowsUpdated = int(num_rec_processed[0][1]) if num_rec_processed[0][1] is not None else 0\n  numTargetRowsDeleted = int(num_rec_processed[0][2]) if num_rec_processed[0][2] is not None else 0\n  numSourceRows = int(num_rec_processed[0][3]) if num_rec_processed[0][3] is not None else 0\n  numOutputRows = int(num_rec_processed[0][4]) if num_rec_processed[0][4] is not None else 0\n    \n  # invoke the activity end date stored procedure\n  activity_end_spoc_statement = f\"\"\"\n    EXEC [audit].[uspActivityEnd] \n    {activityID}, {numSourceRows}, {numTargetRowsInserted}, \n    {numTargetRowsUpdated},{numTargetRowsDeleted}, {numOutputRows}\"\"\"\n    \n  sql_server_end_activity = SqlDB()\n  audit_activity_end = sql_server_end_activity.execute_spoc_without_output(activity_end_spoc_statement)\n    \n  return audit_activity_end\n\n\ndef upd_activity_erorlog(activityID, errorMsg, batchId, addlInfo):\n  # invoke the activity end date stored procedure\n  activity_failed_spoc_statement = f\"\"\"\n    EXEC [audit].[uspErrorLog]\n    {activityID}, '{errorMsg}', {batchId}, '{addlInfo}'\"\"\"\n  \n  sql_server_errorlog = SqlDB()\n  audit_activity_log = sql_server_errorlog.execute_spoc_without_output(activity_failed_spoc_statement)\n  \n  return audit_activity_log\n\n\ndef get_batchId(SourceSystem, SubSystem):\n  # Write your sql statement as a string. Add a '?' for every output parameter.\n  batch_spoc_statement = f\"\"\" EXEC [audit].[uspBatchStart] '{SourceSystem}', '{SubSystem}',? \"\"\"\n  \n  sql_server_batch_start = SqlDB()\n  audit_start_batchId = sql_server_batch_start.execute_spoc_with_output(batch_spoc_statement)\n  \n  return audit_start_batchId\n\ndef get_activityId(PipelineID, PipelineName, BatchID, Phase,\n                   ActivityName, ActivityType, SourceObjectName, UserID):\n  # invoke the activity end date stored procedure\n  activity_start_spoc_statement = f\"\"\"\n    EXEC [audit].[uspActivityStart] \n    '{PipelineID}', '{PipelineName}', {BatchID}, '{Phase}','{ActivityName}', '{ActivityType}', '{SourceObjectName}','{UserID}',?\"\"\"\n  \n  sql_server_activity = SqlDB()\n  audit_start_activityId = sql_server_activity.execute_spoc_with_output(activity_start_spoc_statement)\n  \n  return audit_start_activityId\n\ndef end_batchId(BatchId):\n  # Write your sql statement as a string. Add a '?' for every output parameter.\n  batch_spoc_statement = f\"\"\" EXEC [audit].[uspBatchEnd] {BatchId} \"\"\"\n  \n  sql_server_batch_end = SqlDB()\n  audit_end_batchId = sql_server_batch_end.execute_spoc_without_output(batch_spoc_statement)\n  \n  return audit_end_batchId\n\ndef end_waterMark(WatermarkSubSystem, WatermarkTableName, WatermarkColumn):\n  # Write your sql statement as a string. Add a '?' for every output parameter.\n  watermark_spoc_statement = f\"\"\" EXEC [audit].[uspWatermarkEnd] '{WatermarkSubSystem}', '{WatermarkTableName}','{WatermarkColumn}' \"\"\"\n  \n  sql_server_end_wm = SqlDB()\n  audit_end_wm = sql_server_end_wm.execute_spoc_without_output(watermark_spoc_statement)\n  \n  return audit_end_wm"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2f8e379-552f-4881-8d23-1ed8a764f7ac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions  import date_format\nfrom delta.tables import *\n\n# get BatchId for auditing.\naudit_batchId = get_batchId(p_SourceSystem, p_SubSystem)\n\ndata_itr = config_table_df.rdd.toLocalIterator()\n# looping thorough each row of the dataframe\nfor row in data_itr:\n  if row[\"SourceName\"].upper() == p_SourceSystem:\n    v_SourceObjectName = row[\"SourceDBName\"]+\".\"+row[\"TableName\"]\n    audit_activityId = get_activityId(v_process_start_dtm,v_job_name,  audit_batchId, p_Phase,v_ActivityName, v_ActivityType, v_SourceObjectName, v_user)\n    if file_exists(row[\"SourceLocation\"]):\n      if row[\"WatermarkValue\"] is not None and row[\"ADLMergeKey\"] is not None and row[\"LoadType\"].strip().upper() != c_full_load :\n        try:\n          print('********  Incremental load has begun for '+ row[\"SourceDBName\"]+\".\"+row[\"TableName\"] + ' ********')\n          #Read data from Code orange datalake trusted layer\n          df = spark.read.format(c_src_file_format).load(row[\"SourceLocation\"])\n          audit_col = [col.strip() for col in df.columns if col == row[\"WatermarkColumn\"] ]\n          \n          # using list comprehension\n          load_state_col = ' '.join(map(str, audit_col))\n          filter_value =  '1900-12-31 12:59:59' if row[\"WatermarkValue\"] is None else row[\"WatermarkValue\"]\n          increment_df = df.where(df[load_state_col] > filter_value)\n                    \n          # prepare merge sql\n          # generated joining conditons for merge statement\n          merge_condition_lst =  row[\"ADLMergeKey\"].split(\",\")\n          empty_init_str = ''\n          for colname in merge_condition_lst:\n            empty_init_str = empty_init_str + 't.{}'.format(colname)+ ' = s.{}'.format(colname) + '  AND '\n          \n          # generated joining conditons for merge statement\n          conditional_str = empty_init_str[:-5]                  \n          deltaTable = DeltaTable.forPath(spark, row[\"DestinationLocation\"])\n          \n          # Schema Evolution with a Merge Operation\n          deltaTable.alias(\"t\").merge(\n           increment_df.alias(\"s\"),\n           conditional_str\n           ).whenMatchedUpdateAll(\n           ).whenNotMatchedInsertAll(\n           ).execute()\n          \n          if row[\"StgActive\"] == '1':\n            src_query = row[\"SourceSQL\"].rstrip('\\r').rstrip('\\n')\n            stg_tbl_nm = c_stg_dbname + '.' + row[\"TableName\"]\n            query_df = spark.sql(src_query)\n            query_df = DataFrameWriter(query_df)\n            query_df.jdbc(url=jdbcConnStrUrl, table=stg_tbl_nm, mode=\"overwrite\")\n            \n          # write audit details\n          spoc_end_watermark=end_waterMark(row[\"SubSystem\"],row[\"TableName\"], row[\"WatermarkColumn\"])\n          activity_end_activity = get_row_processed_cnt(row[\"DestinationLocation\"], audit_activityId)\n                    \n        except Exception as e:\n          error_msg=\"ERROR: error in Incremental loading for {subsystem} - {table} table\"+ \":  \" + str(e).format(table=row[\"TableName\"], subsystem=row[\"SubSystem\"])\n          # write audit details\n          activity_fail_spoc_statement = upd_activity_erorlog(activityID=audit_activityId, errorMsg=error_msg, batchId=audit_batchId, addlInfo=None)\n          audit_activity_end = sql_server.execute_spoc_without_output(activity_fail_spoc_statement)\n      else:\n        try:\n          print('********  FULL load has begun for '+ row[\"SourceDBName\"]+\".\"+row[\"TableName\"] + ' ********')\n          #Read data from Code orange datalake trusted layer\n          df = spark.read.format(c_src_file_format).load(row[\"SourceLocation\"])\n          \n          #Create table with path using DataFrame's schema and write data to it\n          target_table_name = row[\"SourceDBName\"]+\".\"+row[\"TableName\"]\n          df.write.format(c_src_file_format).mode(\"overwrite\").option(\"path\", row[\"DestinationLocation\"]).saveAsTable(target_table_name)\n          \n          if row[\"StgActive\"] == '1':\n            src_query =  row[\"SourceSQL\"].rstrip('\\r').rstrip('\\n')\n            stg_tbl_nm = c_stg_dbname + '.' + row[\"TableName\"]\n            query_df = spark.sql(src_query)\n            query_df = DataFrameWriter(query_df)\n            query_df.jdbc(url=jdbcConnStrUrl, table=stg_tbl_nm, mode=\"overwrite\")\n          \n          # write audit details\n          spoc_end_watermark=end_waterMark(row[\"SubSystem\"],row[\"TableName\"], row[\"WatermarkColumn\"])\n          activity_end_activity = get_row_processed_cnt(row[\"DestinationLocation\"], audit_activityId)\n          \n        except Exception as e:\n          error_msg=\"ERROR: error in FULL loading for {subsystem} - {table} table \".format(table=row[\"TableName\"], subsystem=row[\"SubSystem\"]) + \":\" + str(e)\n          # write audit details\n          activity_log = upd_activity_erorlog(activityID=audit_activityId, errorMsg=error_msg, batchId=audit_batchId, addlInfo=None)\n    else:\n      error_msg=\"ERROR: Source File Location-{fileloc} doesn't exist.\".format(fileloc=row[\"SourceLocation\"])\n      # write audit details\n      activity_log = upd_activity_erorlog(activityID=audit_activityId, errorMsg=error_msg, batchId=audit_batchId, addlInfo=None)\n  else:\n    error_msg=\"ERROR: Except source {expct_source} ,others like-{source} isn't supported.\".format(expct_source=p_SourceSystem, source=row[\"SourceName\"])\n    # write audit details\n    activity_log = upd_activity_erorlog(activityID=audit_activityId, errorMsg=error_msg, batchId=audit_batchId, addlInfo=None)\n\nclose_batchId = end_batchId(audit_batchId)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca573f91-ed14-4a46-83b0-acfe1e86ec57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">********  FULL load has begun for t_erp_manufacturing_o2s_rbp_nonconf.MCH1 ********\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">********  FULL load has begun for t_erp.MCHTABLE100 ********\n</div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"copyDeltaTableExecStoredProc","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"p_SourceSystem":{"nuid":"3929952c-9ea2-4864-8107-4a44ccc3f4c5","currentValue":"CO","widgetInfo":{"widgetType":"text","name":"p_SourceSystem","defaultValue":"","label":"Source System Name","options":{"widgetType":"text","validationRegex":null}}},"p_Phase":{"nuid":"2426a251-f19d-4555-8b25-9fbc0fefed6d","currentValue":"RAW","widgetInfo":{"widgetType":"text","name":"p_Phase","defaultValue":"","label":"PhaseName/StageName","options":{"widgetType":"text","validationRegex":null}}},"p_SubSystem":{"nuid":"166313d0-67d9-4e29-a9bc-0b45302e1ed9","currentValue":"RBP","widgetInfo":{"widgetType":"text","name":"p_SubSystem","defaultValue":"","label":"Sub System Name","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":4427844682087154}},"nbformat":4,"nbformat_minor":0}
